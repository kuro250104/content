## Introduction

Les machines Ã  vecteurs de support (SVM) sont des mÃ©thodes d'apprentissage supervisÃ© puissantes et flexibles utilisÃ©es pour la classification, la rÃ©gression et la dÃ©tection des valeurs aberrantes. Les SVM sont trÃ¨s efficaces dans les espaces de grande dimension et sont gÃ©nÃ©ralement utilisÃ©s dans les problÃ¨mes de classification. Les SVM sont populaires et Ã©conomes en mÃ©moire car ils utilisent un sous-ensemble de points d'apprentissage dans la fonction de dÃ©cision.

L'objectif principal des SVM est de diviser les ensembles de donnÃ©es en un certain nombre de classes afin de trouver un **hyperplan marginal maximal (MMH)**, ce qui peut Ãªtre fait en deux Ã©tapes : 1.

- Les machines Ã  vecteurs de support gÃ©nÃ¨rent d'abord itÃ©rativement des hyperplans qui sÃ©parent les classes de la meilleure faÃ§on.
- Ensuite, il choisira l'hyperplan qui sÃ©pare correctement les classes.

Quelques concepts importants dans le SVM sont les suivants .

- **Support Vectors** âˆ’ Ils peuvent Ãªtre dÃ©finis comme les points de donnÃ©es les plus proches de l'hyperplan. Les vecteurs de support aident Ã  dÃ©cider de la ligne de sÃ©paration.
- **Hyperplane** âˆ’ Le plan ou l'espace de dÃ©cision qui divise un ensemble d'objets ayant des classes diffÃ©rentes.
- **Margin** âˆ’ L'Ã©cart entre deux lignes sur les points de donnÃ©es rapprochÃ©s de classes diffÃ©rentes est appelÃ© marge.

Le SVM de Scikit-learn prend en charge les vecteurs d'Ã©chantillons Ã©pars et denses comme entrÃ©e.

## Classification of SVM

Scikit-learn fournit trois classes, Ã  savoir SVC, NuSVC et LinearSVC, qui peuvent effectuer une classification multi-classes.

## SVC

Il s'agit d'une classification vectorielle de support en C dont l'implÃ©mentation est basÃ©e sur libsvm. Le module utilisÃ© par scikit-learn est ```sklearn.svm.SVC```. Cette classe gÃ¨re le support multi classe selon le schÃ©ma one-vs-one.

### ParamÃ¨tres

La liste suivante prÃ©sente les paramÃ¨tres utilisÃ©s par la classe ```sklearn.svm.SVC```.

**ParamÃ¨tre et description**

- **C** âˆ’ float, optional, default = 1.0 - C'est le paramÃ¨tre de pÃ©nalitÃ© du terme d'erreur.
- **kernel** âˆ’ string, optional, default = â€˜rbfâ€™ - Ce paramÃ¨tre spÃ©cifie le type de noyau Ã  utiliser dans l'algorithme. Nous pouvons choisir n'importe lequel parmi 'linear', 'poly', 'rbf', 'sigmoÃ¯de', 'precomputed'. La valeur par dÃ©faut du noyau est "rbf".
- **degree** âˆ’ int, optional, default = 3 - Il reprÃ©sente le degrÃ© de la fonction noyau 'poly' et sera ignorÃ© par tous les autres noyaux.
- **gamma** âˆ’ {â€˜scaleâ€™, â€˜autoâ€™} or float, - Il s'agit du coefficient du noyau pour les noyaux 'rbf', 'poly' et 'sigmoÃ¯de'.
- **optinal default** âˆ’ = â€˜scaleâ€™ - Si vous choisissez la valeur par dÃ©faut, c'est-Ã -dire gamma = 'scale', la valeur du gamma Ã  utiliser par le SVC est 1/(ğ‘›_ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ âˆ—ğ‘‹.ğ‘£ğ‘ğ‘Ÿ()). En revanche, si gamma= 'auto', il utilise 1/ğ‘›_ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ .
- **coef0** âˆ’ float, optional, Default=0.0 - Un terme indÃ©pendant dans la fonction noyau qui n'est significatif que dans 'poly' et 'sigmoÃ¯de'.
- **tol** âˆ’ float, optional, default = 1.e-3 - Ce paramÃ¨tre reprÃ©sente le critÃ¨re d'arrÃªt des itÃ©rations.
- **shrinking** âˆ’ Boolean, optional, default = True - Ce paramÃ¨tre indique si nous voulons utiliser l'heuristique de rÃ©duction ou non.
- **verbose** âˆ’ Boolean, default: false - Il active ou dÃ©sactive la sortie verbeuse. Sa valeur par dÃ©faut est false.
- **probability** âˆ’ boolean, optional, default = true - Ce paramÃ¨tre active ou dÃ©sactive les estimations de probabilitÃ©. La valeur par dÃ©faut est false, mais il doit Ãªtre activÃ© avant d'appeler fit.
- **max_iter** âˆ’ int, optional, default = -1 - Comme son nom l'indique, elle reprÃ©sente le nombre maximum d'itÃ©rations dans le solveur. La valeur -1 signifie qu'il n'y a pas de limite au nombre d'itÃ©rations.
- **cache_size** âˆ’ float, optional - Ce paramÃ¨tre spÃ©cifie la taille du cache du noyau. La valeur sera en MB (MegaBytes).
- **random_state** âˆ’ int, RandomState instance or None, optional, default = none - Ce paramÃ¨tre reprÃ©sente la graine du nombre pseudo-alÃ©atoire gÃ©nÃ©rÃ© qui est utilisÃ© lors du brassage des donnÃ©es. Les options suivantes sont disponibles
    - **int** âˆ’ Dans ce cas, random_state est la graine utilisÃ©e par le gÃ©nÃ©rateur de nombres alÃ©atoires.
    - **RandomState instance** âˆ’ Dans ce cas, random_state est le gÃ©nÃ©rateur de nombres alÃ©atoires.
    - **None** âˆ’ Dans ce cas, le gÃ©nÃ©rateur de nombres alÃ©atoires est l'instance RandonState utilisÃ©e par np.random.
- **class_weight** âˆ’ {dict, â€˜balancedâ€™}, optional - Ce paramÃ¨tre va fixer le paramÃ¨tre C de la classe j Ã  ğ‘ğ‘™ğ‘ğ‘ ğ‘ _ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡[ğ‘—]âˆ—ğ¶ pour le SVC. Si nous utilisons l'option par dÃ©faut, cela signifie que toutes les classes sont censÃ©es avoir le poids un. En revanche, si vous choisissez class_weight:balanced, il utilisera les valeurs de y pour ajuster automatiquement les poids.
- **decision_function_shape** âˆ’ ovoâ€™, â€˜ovrâ€™, default = â€˜ovrâ€™ - Ce paramÃ¨tre dÃ©cidera si l'algorithme retournera la fonction de dÃ©cision 'ovr' (one-vs-rest) de la forme comme tous les autres classificateurs, ou la fonction de dÃ©cision originale ovo(one-vs-one) de libsvm.
- **break_ties** âˆ’ boolean, optional, default = false - 
    - **VÃ©ritable** âˆ’ La prÃ©diction dÃ©partagera les ex-aequo en fonction des valeurs de confiance de la fonction dÃ©cisionnelle.
    - **Faux** âˆ’ Le prÃ©dicteur renverra la premiÃ¨re classe parmi les classes ex aequo.

### Attributs

Followings table consist the attributes used by sklearn.svm.SVC class âˆ’

**Attributs et description**

- **support_** âˆ’ array-like, shape = [n_SV] - Elle renvoie les indices des vecteurs de support.
- **support_vectors_** âˆ’ array-like, shape = [n_SV, n_features] - Elle renvoie les vecteurs de support.
- **n_support_** âˆ’ array-like, dtype=int32, shape = [n_class] - Il reprÃ©sente le nombre de vecteurs de support pour chaque classe.
- **dual_coef_** âˆ’ array, shape = [n_class-1,n_SV] - Ce sont les coefficients des vecteurs de support dans la fonction de dÃ©cision.
- **coef_** âˆ’ array, shape = [n_class * (n_class-1)/2, n_features] - Cet attribut, disponible uniquement en cas de noyau linÃ©aire, fournit le poids attribuÃ© aux caractÃ©ristiques.
- **intercept_** âˆ’ array, shape = [n_class * (n_class-1)/2] - Il reprÃ©sente le terme indÃ©pendant (constante) dans la fonction de dÃ©cision.
- **fit_status_** âˆ’ int - La sortie serait 0 si elle est correctement installÃ©e. La sortie sera de 1 si elle est incorrectement installÃ©e.
- **classes_** âˆ’ array of shape = [n_classes] - Il donne les Ã©tiquettes des classes.

### Exemple de mise en Å“uvre

Comme d'autres classificateurs, le SVC doit Ã©galement Ãªtre Ã©quipÃ© des deux tableaux suivants -

- Un tableau X contenant les Ã©chantillons d'apprentissage. Il est de taille [n_samples, n_features]. 
- Un tableau Y contenant les valeurs cibles, c'est-Ã -dire les Ã©tiquettes de classe pour les Ã©chantillons d'apprentissage. Il est de taille [n_Ã©chantillons].

Le script Python suivant utilise la classe sklearn.svm.SVC.

```python
import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
SVCClf = SVC(kernel = 'linear',gamma = 'scale', shrinking = False,)
SVCClf.fit(X, y)
```

### Sortie

```python
SVC(C = 1.0, cache_size = 200, class_weight = None, coef0 = 0.0,
    decision_function_shape = 'ovr', degree = 3, gamma = 'scale', kernel = 'linear',
    max_iter = -1, probability = False, random_state = None, shrinking = False,
    tol = 0.001, verbose = False)
```

### Exemple

Maintenant, une fois ajustÃ©, nous pouvons obtenir le vecteur de poids Ã  l'aide du script python suivant -.

```python
SVCClf.coef_
```

### Sortie

```python
array([[0.5, 0.5]])
```

### Exemple

De la mÃªme maniÃ¨re, nous pouvons obtenir la valeur des autres attributs comme suit -

```python
SVCClf.predict([[-0.5,-0.8]])
```

### Sortie

```python
array([1])
```

### Exemple

```python
SVCClf.n_support_
```

### Sortie

```python
array([1, 1])
```

### Exemple

```python
SVCClf.support_vectors_
```

### Sortie

```python
array(
    [
        [-1., -1.],
        [ 1., 1.]
    ]
)
```

### Exemple

```python
SVCClf.support_
```

### Sortie

```python
array([0, 2])
```

### Exemple

```python
SVCClf.intercept_
```

### Sortie

```python
array([-0.])
```

### Exemple

```python
SVCClf.fit_status_
```

### Sortie

```python
0
```

## NuSVC

NuSVC est Nu Support Vector Classification. C'est une autre classe fournie par scikit-learn qui peut effectuer une classification multi-classes. C'est comme la SVC mais NuSVC accepte des jeux de paramÃ¨tres lÃ©gÃ¨rement diffÃ©rents. Le paramÃ¨tre qui est diffÃ©rent de SVC est le suivant : ```nu - flotteur, facultatif, par dÃ©faut = 0,5```

Il reprÃ©sente une limite supÃ©rieure de la fraction des erreurs de formation et une limite infÃ©rieure de la fraction des vecteurs de support. Sa valeur doit Ãªtre comprise dans l'intervalle (o,1). Les autres paramÃ¨tres et attributs sont les mÃªmes que ceux du SVC.

### Exemple de mise en Å“uvre

Nous pouvons Ã©galement mettre en Å“uvre le mÃªme exemple en utilisant la classe ```sklearn.svm.NuSVC```.

```python
import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
y = np.array([1, 1, 2, 2])
from sklearn.svm import NuSVC
NuSVCClf = NuSVC(kernel = 'linear',gamma = 'scale', shrinking = False,)
NuSVCClf.fit(X, y)
```

### Sortie

```python
NuSVC(cache_size = 200, class_weight = None, coef0 = 0.0,
    decision_function_shape = 'ovr', degree = 3, gamma = 'scale', kernel = 'linear',
    max_iter = -1, nu = 0.5, probability = False, random_state = None,
    shrinking = False, tol = 0.001, verbose = False)
```

Nous pouvons obtenir les sorties du reste des attributs comme dans le cas du SVC.

## LinearSVC

Il s'agit de la classification linÃ©aire par vecteurs de support. Elle est similaire Ã  la SVC avec un noyau = 'linear'. La diffÃ©rence entre eux est que LinearSVC est implÃ©mentÃ© en termes de liblinear alors que SVC est implÃ©mentÃ© en libsvm. C'est la raison pour laquelle LinearSVC a plus de flexibilitÃ© dans le choix des pÃ©nalitÃ©s et des fonctions de perte. Il s'adapte Ã©galement mieux Ã  un grand nombre d'Ã©chantillons.

Si nous parlons de ses paramÃ¨tres et attributs, il ne prend pas en charge le "noyau" car il est supposÃ© Ãªtre linÃ©aire et il lui manque Ã©galement certains attributs comme support_, support_vectors_, n_support_, fit_status_ et dual_coef_.

Cependant, il prend en charge les paramÃ¨tres de pÃ©nalitÃ© et de perte comme suit -

- **penalty** - chaÃ®ne de caractÃ¨res, L1 ou L2(default = 'L2') -  Ce paramÃ¨tre est utilisÃ© pour spÃ©cifier la norme (L1 ou L2) utilisÃ©e dans la pÃ©nalisation (rÃ©gularisation).
- **loss** - string, hinge, squared_hinge (default = squared_hinge) - Il reprÃ©sente la fonction de perte oÃ¹ 'hinge' est la perte SVM standard et 'squared_hinge' est le carrÃ© de la perte hinge.

### Exemple de mise en Å“uvre

Le script Python suivant utilise la classe sklearn.svm.LinearSVC.

```python
from sklearn.svm import LinearSVC
from sklearn.datasets import make_classification
X, y = make_classification(n_features = 4, random_state = 0)
LSVCClf = LinearSVC(dual = False, random_state = 0, penalty = 'l1',tol = 1e-5)
LSVCClf.fit(X, y)
```

### Sortie

```python
LinearSVC(C = 1.0, class_weight = None, dual = False, fit_intercept = True,
    intercept_scaling = 1, loss = 'squared_hinge', max_iter = 1000,
    multi_class = 'ovr', penalty = 'l1', random_state = 0, tol = 1e-05, verbose = 0)
```

### Exemple

Maintenant, une fois ajustÃ©, le modÃ¨le peut prÃ©dire de nouvelles valeurs comme suit -

```python
LSVCClf.predict([[0,0,0,0]])
```

### Sortie

```python
[1]
```

### Exemple

Pour l'exemple ci-dessus, nous pouvons obtenir le vecteur poids Ã  l'aide du script python suivant -

```python
LSVCClf.coef_
```

### Sortie

```python
[[0. 0. 0.91214955 0.22630686]]
```

### Exemple

De mÃªme, nous pouvons obtenir la valeur d'intercept Ã  l'aide du script python suivant -

```python
LSVCClf.intercept_
```

### Sortie

```python
[0.26860518]
```

## Regression with SVM

Comme nous l'avons vu prÃ©cÃ©demment, le SVM est utilisÃ© pour les problÃ¨mes de classification et de rÃ©gression. La mÃ©thode de classification par vecteurs de support (SVC) de Scikit-learn peut Ãªtre Ã©tendue pour rÃ©soudre Ã©galement les problÃ¨mes de rÃ©gression. Cette mÃ©thode Ã©tendue est appelÃ©e RÃ©gression par vecteur de support (SVR).

### Similitude de base entre SVM et SVR

Le modÃ¨le crÃ©Ã© par le SVC ne dÃ©pend que d'un sous-ensemble de donnÃ©es d'entraÃ®nement. Pourquoi ? Parce que la fonction de coÃ»t pour la construction du modÃ¨le ne se soucie pas des points de donnÃ©es d'apprentissage qui se trouvent en dehors de la marge.

Le modÃ¨le produit par le SVR (Support Vector Regression) ne dÃ©pend lui aussi que d'un sous-ensemble de donnÃ©es d'apprentissage. Pourquoi ? Parce que la fonction de coÃ»t pour construire le modÃ¨le ignore tous les points de donnÃ©es d'apprentissage proches de la prÃ©diction du modÃ¨le.

Scikit-learn fournit trois classes, Ã  savoir SVR, NuSVR et LinearSVR, qui sont trois implÃ©mentations diffÃ©rentes de SVR.

## SVR

Il s'agit d'une rÃ©gression par vecteur support Epsilon dont l'implÃ©mentation est basÃ©e sur libsvm. Contrairement Ã  SVC, il y a deux paramÃ¨tres libres dans le modÃ¨le, Ã  savoir 'C' et 'epsilon'.

- **epsilon** - float, optionnel, par dÃ©faut = 0.1

Il reprÃ©sente l'epsilon dans le modÃ¨le epsilon-SVR, et spÃ©cifie le tube epsilon Ã  l'intÃ©rieur duquel aucune pÃ©nalitÃ© n'est associÃ©e dans la fonction de perte d'apprentissage aux points prÃ©dits Ã  une distance epsilon de la valeur rÃ©elle.

Le reste des paramÃ¨tres et attributs sont similaires Ã  ceux que nous avons utilisÃ©s dans le SVC.

### Exemple de mise en Å“uvre

Le script Python suivant utilise la classe sklearn.svm.SVR.

```python
from sklearn import svm
X = [[1, 1], [2, 2]]
y = [1, 2]
SVRReg = svm.SVR(kernel = 'linear', gamma = 'auto')
SVRReg.fit(X, y)
```

### Sortie

```python
SVR(C = 1.0, cache_size = 200, coef0 = 0.0, degree = 3, epsilon = 0.1, gamma = 'auto',
    kernel = 'linear', max_iter = -1, shrinking = True, tol = 0.001, verbose = False)
```

### Exemple

Maintenant, une fois ajustÃ©, nous pouvons obtenir le vecteur de poids Ã  l'aide du script python suivant -.

```python
SVRReg.coef_
```

### Sortie

```python
array([[0.4, 0.4]])
```

### Exemple

De la mÃªme maniÃ¨re, nous pouvons obtenir la valeur des autres attributs comme suit -

```python
SVRReg.predict([[1,1]])
```

### Sortie

```python
array([1.1])
```

De mÃªme, nous pouvons obtenir les valeurs d'autres attributs.

## NuSVR

NuSVR est Nu Support Vector Regression. C'est comme NuSVC, mais NuSVR utilise un paramÃ¨tre nu pour contrÃ´ler le nombre de vecteurs de support. Et de plus, contrairement Ã  NuSVC oÃ¹ nu remplace le paramÃ¨tre C, ici il remplace epsilon.

### Exemple de mise en Å“uvre

Le script Python suivant utilise la classe sklearn.svm.SVR.

```python
from sklearn.svm import NuSVR
import numpy as np
n_samples, n_features = 20, 15
np.random.seed(0)
y = np.random.randn(n_samples)
X = np.random.randn(n_samples, n_features)
NuSVRReg = NuSVR(kernel = 'linear', gamma = 'auto',C = 1.0, nu = 0.1)^M
NuSVRReg.fit(X, y)
```

### Sortie

```python
NuSVR(C = 1.0, cache_size = 200, coef0 = 0.0, degree = 3, gamma = 'auto',
    kernel = 'linear', max_iter = -1, nu = 0.1, shrinking = True, tol = 0.001,
    verbose = False)
```

### Exemple

Maintenant, une fois ajustÃ©, nous pouvons obtenir le vecteur de poids Ã  l'aide du script python suivant -.

```python
NuSVRReg.coef_
```

### Sortie

```python
array(
    [
        [-0.14904483, 0.04596145, 0.22605216, -0.08125403, 0.06564533,
        0.01104285, 0.04068767, 0.2918337 , -0.13473211, 0.36006765,
        -0.2185713 , -0.31836476, -0.03048429, 0.16102126, -0.29317051]
    ]
)
```

De mÃªme, nous pouvons obtenir la valeur d'autres attributs.

## LinearSVR

Il s'agit de la rÃ©gression linÃ©aire par vecteur de support. Elle est similaire Ã  la SVR avec un noyau = 'linÃ©aire'. La diffÃ©rence entre eux est que LinearSVR est implÃ©mentÃ© en termes de liblinear, alors que SVC est implÃ©mentÃ© en libsvm. C'est la raison pour laquelle LinearSVR a plus de flexibilitÃ© dans le choix des pÃ©nalitÃ©s et des fonctions de perte. Il s'adapte Ã©galement mieux Ã  un grand nombre d'Ã©chantillons.

Si nous parlons de ses paramÃ¨tres et attributs, il ne prend pas en charge le "noyau" car il est supposÃ© Ãªtre linÃ©aire et il lui manque Ã©galement certains attributs comme support_, support_vectors_, n_support_, fit_status_ et dual_coef_.

Toutefois, il prend en charge les paramÃ¨tres de "perte" comme suit

- **loss** - chaÃ®ne de caractÃ¨res, facultatif, par dÃ©faut = 'epsilon_insensitive'.

Il reprÃ©sente la fonction de perte oÃ¹ la perte epsilon_insensible est la perte L1 et la perte epsilon-insensible au carrÃ© est la perte L2.

### Exemple de mise en Å“uvre

Le script Python suivant utilise la classe sklearn.svm.LinearSVR.

```python
from sklearn.svm import LinearSVR
from sklearn.datasets import make_regression
X, y = make_regression(n_features = 4, random_state = 0)
LSVRReg = LinearSVR(dual = False, random_state = 0,
loss = 'squared_epsilon_insensitive',tol = 1e-5)
LSVRReg.fit(X, y)
```

### Sortie

```python
LinearSVR(
    C=1.0, dual=False, epsilon=0.0, fit_intercept=True,
    intercept_scaling=1.0, loss='squared_epsilon_insensitive',
    max_iter=1000, random_state=0, tol=1e-05, verbose=0
)
```

### Exemple

Maintenant, une fois ajustÃ©, le modÃ¨le peut prÃ©dire de nouvelles valeurs comme suit -

```python
LSRReg.predict([[0,0,0,0]])
```

### Sortie

```python
array([-0.01041416])
```

### Exemple

Pour l'exemple ci-dessus, nous pouvons obtenir le vecteur poids Ã  l'aide du script python suivant -

```python
LSRReg.coef_
```

### Sortie

```python
array([20.47354746, 34.08619401, 67.23189022, 87.47017787])
```

### Exemple

De mÃªme, nous pouvons obtenir la valeur d'intercept Ã  l'aide du script python suivant -

```python
LSRReg.intercept_
```

### Sortie

```python
array([-0.01041416])
```